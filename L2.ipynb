{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4cd0221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "seed = 5691\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "NUM_OF_CATEGORY = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f101b208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 12:12:30.600268: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-02-23 12:12:30.600373: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# mnist = tf.keras.datasets.fashion_mnist\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = np.reshape(x_train / 255.0, (int(tf.shape(x_train)[0]),-1)), np.reshape(x_test / 255.0, (int(tf.shape(x_test)[0]),-1))\n",
    "one_hot_y = lambda t: [1 if i == t else 0 for i in range(NUM_OF_CATEGORY)]\n",
    "y_train, y_test = np.array([one_hot_y(y) for y in y_train]), np.array([one_hot_y(y) for y in y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e6811c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self, size_input, size_hidden, size_output, device=None):\n",
    "        super(MLP, self).__init__()\n",
    "        \"\"\"\n",
    "        size_input: int, size of input layer\n",
    "        size_hidden: int, size of hidden layer\n",
    "        size_output: int, size of output layer\n",
    "        device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
    "        \"\"\"\n",
    "        self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
    "        size_input, size_hidden, size_output, device\n",
    "\n",
    "        # Initialize weights between input layer and hidden layer 1\n",
    "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden], seed=seed, stddev=0.1))\n",
    "        # Initialize biases for hidden layer 1\n",
    "        self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden], seed=seed))\n",
    "         # Initialize weights between hidden layer 1 and hidden layer 2\n",
    "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden, self.size_hidden], seed=seed, stddev=0.1))\n",
    "        # Initialize biases for hidden layer 2\n",
    "        self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden], seed=seed))\n",
    "         # Initialize weights between hidden layer 2 and output layer\n",
    "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden, self.size_output], seed=seed, stddev=0.1))\n",
    "        # Initialize biases for output layer\n",
    "        self.b3 = tf.Variable(tf.random.normal([1, self.size_output], seed=seed))\n",
    "\n",
    "        # Define variables to be updated during backpropagation\n",
    "        self.MLP_variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
    "        # self.MLP_variables = [self.W1, self.W3, self.b1, self.b3]\n",
    "\n",
    "    \n",
    "    def forward(self, X, dropout_rate=0):\n",
    "        \"\"\"\n",
    "        forward pass\n",
    "        X: Tensor, inputs\n",
    "        \"\"\"\n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X, dropout_rate=dropout_rate)\n",
    "        else:\n",
    "            self.y = self.compute_output(X, dropout_rate=dropout_rate)\n",
    "\n",
    "        return self.y\n",
    "  \n",
    "    def loss(self, y_pred, y_true, L1=0, L2=0):\n",
    "        '''\n",
    "        y_pred - Tensor of shape (batch_size, size_output)\n",
    "        y_true - Tensor of shape (batch_size, size_output)\n",
    "        '''\n",
    "        y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
    "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "        l2_penlty = (tf.nn.l2_loss(self.W1)+tf.nn.l2_loss(self.W2)+tf.nn.l2_loss(self.W3))*L2\n",
    "        loss_with_l2 = l2_penlty+tf.losses.categorical_crossentropy(y_true_tf, y_pred_tf)\n",
    "        return loss_with_l2\n",
    "  \n",
    "    def backward(self, X_train, y_train, dropout_rate, learning_rate, L1=0, L2=0):\n",
    "        \"\"\"\n",
    "        backward pass\n",
    "        \"\"\"\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train,dropout_rate=dropout_rate)\n",
    "            current_loss = self.loss(predicted, y_train, L2=L2)\n",
    "        grads = tape.gradient(current_loss, self.MLP_variables)\n",
    "        optimizer.apply_gradients(zip(grads, self.MLP_variables))\n",
    "        return current_loss, predicted\n",
    "        \n",
    "        \n",
    "    def compute_output(self, X, dropout_rate=0):\n",
    "        \"\"\"\n",
    "        Custom method to obtain output tensor during forward pass\n",
    "        \"\"\"\n",
    "        # Cast X to float32\n",
    "        X_tf = tf.cast(X, dtype=tf.float32)\n",
    "        #Remember to normalize your dataset before moving forward\n",
    "        # Compute values in hidden layer 1\n",
    "        what = tf.matmul(X_tf, self.W1) + self.b1\n",
    "        hhat = tf.nn.relu(what)\n",
    "        # Implement Dropout\n",
    "        hhat = tf.nn.dropout(hhat, rate = dropout_rate, seed=seed)\n",
    "\n",
    "        # Compute values in hidden layer 2\n",
    "        what = tf.matmul(hhat, self.W2) + self.b2\n",
    "        hhat = tf.nn.relu(what)\n",
    "        # Implement Dropout\n",
    "        hhat = tf.nn.dropout(hhat, rate = dropout_rate, seed=seed)\n",
    "\n",
    "        # Compute output\n",
    "        output = tf.matmul(hhat, self.W3) + self.b3\n",
    "        #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
    "        #Second add tf.Softmax(output) and then return this variable\n",
    "        return tf.nn.softmax(output)\n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17be0786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 !!!!!!!!!!!!!!!!!\n",
      "Number of Epoch = 1 - Training Cross Entropy:= 0.7741124348958334 - Training Accuracy:= 0.9221500158309937 - Test Accuracy:= 0.9230000376701355\n",
      "Time taken (in seconds): 5.94\n",
      "Number of Epoch = 2 - Training Cross Entropy:= 0.41468828125 - Training Accuracy:= 0.9477166533470154 - Test Accuracy:= 0.9470000267028809\n",
      "Time taken (in seconds): 5.97\n",
      "Number of Epoch = 3 - Training Cross Entropy:= 0.33448977864583335 - Training Accuracy:= 0.9547333717346191 - Test Accuracy:= 0.9539000391960144\n",
      "Time taken (in seconds): 5.91\n",
      "Number of Epoch = 4 - Training Cross Entropy:= 0.28856842447916664 - Training Accuracy:= 0.9576166868209839 - Test Accuracy:= 0.954200029373169\n",
      "Time taken (in seconds): 5.91\n",
      "Number of Epoch = 5 - Training Cross Entropy:= 0.25848089192708334 - Training Accuracy:= 0.965149998664856 - Test Accuracy:= 0.9622000455856323\n",
      "Time taken (in seconds): 5.89\n",
      "Number of Epoch = 6 - Training Cross Entropy:= 0.2387244140625 - Training Accuracy:= 0.9670667052268982 - Test Accuracy:= 0.96260005235672\n",
      "Time taken (in seconds): 5.84\n",
      "Number of Epoch = 7 - Training Cross Entropy:= 0.22554567057291666 - Training Accuracy:= 0.9680500030517578 - Test Accuracy:= 0.9620000720024109\n",
      "Time taken (in seconds): 5.85\n",
      "Number of Epoch = 8 - Training Cross Entropy:= 0.21350934244791667 - Training Accuracy:= 0.968666672706604 - Test Accuracy:= 0.9633000493049622\n",
      "Time taken (in seconds): 5.85\n",
      "Number of Epoch = 9 - Training Cross Entropy:= 0.20536507161458334 - Training Accuracy:= 0.9717000126838684 - Test Accuracy:= 0.9671000242233276\n",
      "Time taken (in seconds): 5.89\n",
      "Number of Epoch = 10 - Training Cross Entropy:= 0.19946175130208332 - Training Accuracy:= 0.9653500318527222 - Test Accuracy:= 0.9586000442504883\n",
      "Time taken (in seconds): 5.83\n",
      "Number of Epoch = 11 - Training Cross Entropy:= 0.19554552408854167 - Training Accuracy:= 0.9657999873161316 - Test Accuracy:= 0.9622000455856323\n",
      "Time taken (in seconds): 5.83\n",
      "Number of Epoch = 12 - Training Cross Entropy:= 0.19186720377604166 - Training Accuracy:= 0.9730333685874939 - Test Accuracy:= 0.9671000242233276\n",
      "Time taken (in seconds): 5.83\n",
      "Number of Epoch = 13 - Training Cross Entropy:= 0.187900146484375 - Training Accuracy:= 0.9722833633422852 - Test Accuracy:= 0.9665000438690186\n",
      "Time taken (in seconds): 5.86\n",
      "Number of Epoch = 14 - Training Cross Entropy:= 0.18452897135416665 - Training Accuracy:= 0.9740666747093201 - Test Accuracy:= 0.966200053691864\n",
      "Time taken (in seconds): 5.86\n",
      "Number of Epoch = 15 - Training Cross Entropy:= 0.18376790364583334 - Training Accuracy:= 0.972350001335144 - Test Accuracy:= 0.9687000513076782\n",
      "Time taken (in seconds): 5.90\n",
      "1 !!!!!!!!!!!!!!!!!\n",
      "Number of Epoch = 1 - Training Cross Entropy:= 0.7991009114583333 - Training Accuracy:= 0.9160833358764648 - Test Accuracy:= 0.9184000492095947\n",
      "Time taken (in seconds): 5.88\n",
      "Number of Epoch = 2 - Training Cross Entropy:= 0.42673466796875 - Training Accuracy:= 0.9453833699226379 - Test Accuracy:= 0.9473000168800354\n",
      "Time taken (in seconds): 5.86\n",
      "Number of Epoch = 3 - Training Cross Entropy:= 0.3439193359375 - Training Accuracy:= 0.9528999924659729 - Test Accuracy:= 0.9529000520706177\n",
      "Time taken (in seconds): 5.85\n",
      "Number of Epoch = 4 - Training Cross Entropy:= 0.2949041015625 - Training Accuracy:= 0.9575166702270508 - Test Accuracy:= 0.9561000466346741\n",
      "Time taken (in seconds): 5.83\n",
      "Number of Epoch = 5 - Training Cross Entropy:= 0.262918994140625 - Training Accuracy:= 0.963533341884613 - Test Accuracy:= 0.9624000191688538\n",
      "Time taken (in seconds): 5.90\n",
      "Number of Epoch = 6 - Training Cross Entropy:= 0.24263230794270832 - Training Accuracy:= 0.9657000303268433 - Test Accuracy:= 0.9618000388145447\n",
      "Time taken (in seconds): 5.93\n",
      "Number of Epoch = 7 - Training Cross Entropy:= 0.22850944010416666 - Training Accuracy:= 0.9686999917030334 - Test Accuracy:= 0.9629000425338745\n",
      "Time taken (in seconds): 6.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tk/4gvzt__j4rdgq38_t00zww0r0000gn/T/ipykernel_22045/3977056029.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mcur_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_on_gpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDROPOUT_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL2_PENLTY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mloss_total_gpu\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcur_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0;31m# Calculate Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/tk/4gvzt__j4rdgq38_t00zww0r0000gn/T/ipykernel_22045/3700987882.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, X_train, y_train, dropout_rate, learning_rate, L1, L2)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMLP_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMLP_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/tk/4gvzt__j4rdgq38_t00zww0r0000gn/T/ipykernel_22045/3700987882.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, y_pred, y_true, L1, L2)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         '''\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0my_true_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0my_pred_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0ml2_penlty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mL2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, dtype, name)\u001b[0m\n\u001b[1;32m   1002\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Casting complex to real discards imaginary part.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, DstT, Truncate, name)\u001b[0m\n\u001b[1;32m   1995\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1996\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1997\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   1998\u001b[0m         _ctx, \"Cast\", name, x, \"DstT\", DstT, \"Truncate\", Truncate)\n\u001b[1;32m   1999\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_for_ten = []\n",
    "for s in range(10):\n",
    "    print(s,\"!!!!!!!!!!!!!!!!!\")\n",
    "    DROPOUT_RATE = 0\n",
    "    BATCH_SIZE = 200\n",
    "    HIDDEN_SIZE = 128\n",
    "    NUM_EPOCHS = 15\n",
    "    LEARNING_RATE = 0.001\n",
    "    L2_PENLTY = 0.001\n",
    "\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    path = f\"SEED_{s}-LearnRate_{LEARNING_RATE}-L2_{L2_PENLTY}-DROPOUT_{DROPOUT_RATE}-BATCH_{BATCH_SIZE}-HIDDEN_{HIDDEN_SIZE}-TIME_{current_time}\"\n",
    "\n",
    "    # Set log summary\n",
    "\n",
    "#     train_log_dir = 'logs/mnist_fashion/' + path + '/train'\n",
    "#     test_log_dir = 'logs/mnist_fashion/' + path + '/test'\n",
    "    train_log_dir = 'logs/mnist/' + path + '/train'\n",
    "    test_log_dir = 'logs/mnist/' + path + '/test'\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "\n",
    "    size_input = int(tf.shape(x_train)[1])\n",
    "    size_hidden = HIDDEN_SIZE\n",
    "    size_output = int(tf.shape(y_train)[1])\n",
    "    number_of_train_examples = int(tf.shape(x_train)[0])\n",
    "    number_of_test_examples = int(tf.shape(x_test)[0])\n",
    "\n",
    "\n",
    "    # print(\"size_input\",size_input)\n",
    "    # print(\"size_output\",size_output)\n",
    "    # print(\"number_of_train_examples\",number_of_train_examples)\n",
    "    # print(\"number_of_test_examples\",number_of_test_examples)\n",
    "\n",
    "\n",
    "    mlp_on_gpu = MLP(size_input, size_hidden, size_output, device='cpu')\n",
    "    time_start = time.time()\n",
    "    epoch = 1\n",
    "    loss_diff,last_loss = 1,0\n",
    "\n",
    "    while epoch <= NUM_EPOCHS and abs(loss_diff) > 0.00001:\n",
    "        loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(BATCH_SIZE+BATCH_SIZE//4, seed=epoch*(seed)).batch(BATCH_SIZE)\n",
    "        for inputs, outputs in train_ds:\n",
    "            cur_loss, preds = mlp_on_gpu.backward(inputs, outputs, dropout_rate=DROPOUT_RATE, learning_rate=LEARNING_RATE, L2=L2_PENLTY)\n",
    "            loss_total_gpu += cur_loss\n",
    "      # Calculate Accuracy\n",
    "        train_accuracy, test_accuracy = tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.CategoricalAccuracy()\n",
    "        train_accuracy.update_state(y_train, mlp_on_gpu.forward(x_train))\n",
    "        test_accuracy.update_state(y_test, mlp_on_gpu.forward(x_test))\n",
    "        train_loss = np.sum(loss_total_gpu) / x_train.shape[0]\n",
    "        test_loss = np.sum(mlp_on_gpu.loss(mlp_on_gpu.forward(x_test), y_test)) / x_test.shape[0]\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss, step=epoch)\n",
    "            tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss, step=epoch)\n",
    "            tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "\n",
    "        loss_diff = train_loss - last_loss\n",
    "        last_loss = train_loss\n",
    "        print(f'Number of Epoch = {epoch} - Training Cross Entropy:= {np.sum(loss_total_gpu) / x_train.shape[0]} - Training Accuracy:= {train_accuracy.result().numpy()} - Test Accuracy:= {test_accuracy.result().numpy()}')\n",
    "        time_taken = time.time() - time_start\n",
    "        print('Time taken (in seconds): {:.2f}'.format(time_taken))\n",
    "        time_start = time.time()\n",
    "        epoch += 1\n",
    "    loss_for_ten.append(last_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48ee3abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7660436848958333,\n",
       " 0.626580859375,\n",
       " 0.5297130859375,\n",
       " 9.730785416666667,\n",
       " 9.997573958333334,\n",
       " 6.594867708333333,\n",
       " 0.5322341796875,\n",
       " 0.6875259114583333,\n",
       " 0.5513381510416666,\n",
       " 0.5688837239583333]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_for_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b034ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
